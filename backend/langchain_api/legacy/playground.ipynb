{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=KcDlfGF8rRr3VBfitHuJNPOqh8GDjW&access_type=offline&code_challenge=tfarO2rM4qcHJNJLxGWCTuA0OnmNFzfC1ZIFpwp2TW0&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/till/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"hackathum24mun-28\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "!/Users/till/Downloads/google-cloud-sdk/bin/gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/Users/till/Downloads/google-cloud-sdk/bin/gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, SafetySetting, Part, Content\n",
    "\n",
    "vertexai.init(project=\"hackathum24mun-28\", location=\"europe-west3\")\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-002\",\n",
    ")\n",
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(contents=[\n",
    "    Content(role=\"user\", parts=[Part.from_text(\"Say yes.\")])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to your Life Planning Advisor!\n",
      "Let's discuss your future housing needs and life plans.\n",
      "(Type 'exit', 'quit', 'done', or 'next' when you're ready to move to the preference summary)\n",
      "\n",
      "Advisor: Okay, let's begin planning your future housing journey.  To start, I have two questions to help us frame things:\n",
      "\n",
      "1. **What is your current living situation and what are your immediate housing needs?**  (For example:  Are you renting an apartment? Owning a house? Sharing a house? Do you need more space? Are there any immediate repairs or maintenance issues?)\n",
      "\n",
      "2. **Where do you see yourself professionally and personally in the next 5-10 years?** (This could include career aspirations, relationship goals, family plans, etc.  Be as specific or broad as you feel comfortable.)\n",
      "\n",
      "\n",
      "Once we've established a baseline, we can delve deeper into other factors.  I want to understand not just what you *think* you want in a home, but what will truly support your overall well-being and aspirations over the next decade.  We'll explore all aspects of your life to ensure your housing choice aligns with your evolving needs.  Don't hesitate to share even seemingly insignificant details â€“ they can often paint a clearer picture of your long-term needs.  We'll take our time and explore this together.  Let's start with those first two questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentType, Tool, initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-002\",\n",
    ")\n",
    "\n",
    "# Create memory for the conversation\n",
    "conversation_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Define the initial conversation prompt\n",
    "life_planning_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\"],\n",
    "    template=\"\"\"You are a professional life planning advisor helping people make informed decisions about their future housing choices. Your role is to guide them through a comprehensive discussion about their life plans for the next 10 years, helping them uncover their true needs and preferences beyond their initial assumptions.\n",
    "\n",
    "Start with these two questions:\n",
    "1. What are your current living situation and immediate housing needs?\n",
    "2. Where do you see yourself professionally and personally in the next 5-10 years?\n",
    "\n",
    "After these initial questions, engage in a natural conversation that explores various life aspects relevant to housing choices, such as:\n",
    "- Career growth and workplace location\n",
    "- Family planning (marriage, children, pets)\n",
    "- Lifestyle preferences (urban/suburban, social life, cultural activities)\n",
    "- Daily necessities (transportation, shopping, healthcare)\n",
    "- Educational needs (schools, universities)\n",
    "- Work-life balance\n",
    "- Budget and financial planning\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "Respond professionally and empathetically. Help users discover what they truly need, not just what they think they want.\"\"\"\n",
    ")\n",
    "\n",
    "# Create a preference extraction agent\n",
    "preference_extractor_prompt = PromptTemplate(\n",
    "    input_variables=[\"conversation\"],\n",
    "    template=\"\"\"Based on the following conversation, extract and summarize the key preferences and requirements for housing:\n",
    "\n",
    "{conversation}\n",
    "\n",
    "Please provide a concise summary of:\n",
    "1. Must-have features\n",
    "2. Location preferences\n",
    "3. Lifestyle requirements\n",
    "4. Budget constraints\n",
    "5. Long-term considerations\n",
    "\n",
    "Format the response as a clear, structured summary.\"\"\"\n",
    ")\n",
    "\n",
    "def get_user_input():\n",
    "    return input(\"You: \")\n",
    "\n",
    "def chat_with_user():\n",
    "    chat_history = []\n",
    "    \n",
    "    # Initialize conversation with the life planning prompt\n",
    "    while True:\n",
    "        # Get the next prompt\n",
    "        prompt = life_planning_prompt.format(chat_history=\"\\n\".join(chat_history))\n",
    "        \n",
    "        # Get AI response\n",
    "        response = model.generate_content(\n",
    "            contents=[Content(role=\"user\", parts=[Part.from_text(prompt)])]\n",
    "        ).text\n",
    "        print(\"\\nAdvisor:\", response)\n",
    "        \n",
    "        # Get user input\n",
    "        user_input = get_user_input()\n",
    "        \n",
    "        # Check if user wants to end conversation\n",
    "        if user_input.lower() in ['exit', 'quit', 'done', 'next']:\n",
    "            break\n",
    "            \n",
    "        # Update chat history\n",
    "        chat_history.extend([f\"User: {user_input}\", f\"Advisor: {response}\"])\n",
    "    \n",
    "    return chat_history\n",
    "\n",
    "def extract_preferences(conversation):\n",
    "    prompt = preference_extractor_prompt.format(conversation=\"\\n\".join(conversation))\n",
    "    summary = model.generate_content(\n",
    "        contents=[Content(role=\"user\", parts=[Part.from_text(prompt)])]\n",
    "    ).text\n",
    "    return summary\n",
    "\n",
    "# Main execution flow\n",
    "print(\"Welcome to your Life Planning Advisor!\")\n",
    "print(\"Let's discuss your future housing needs and life plans.\")\n",
    "print(\"(Type 'exit', 'quit', 'done', or 'next' when you're ready to move to the preference summary)\")\n",
    "\n",
    "# Start the conversation\n",
    "conversation_history = chat_with_user()\n",
    "\n",
    "# Extract and display preferences\n",
    "print(\"\\nBased on our conversation, here's a summary of your preferences and requirements:\")\n",
    "preferences_summary = extract_preferences(conversation_history)\n",
    "print(preferences_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackatum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
